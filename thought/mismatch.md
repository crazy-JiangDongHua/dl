## Mismatch: 训练集和测试集分布不一致

训练集高分，测试集预测提交后发现分数很低，为什么？除了模型过拟合训练集的可能，还有可能是训练集和测试集分布不一致



#### 发生原因

最常见的有两种原因[1]：

- 样本选择偏差(Sample Selection Bias): 训练集是通过有偏方法得到的，例如非均匀选择(Non-uniform Selection)，导致训练集无法很好表征的真实样本空间。
- 环境不平稳(Non-stationary Environments): 当训练集数据的采集环境跟测试集不一致时会出现该问题，一般是由于时间或空间的改变引起的。



#### 判断方法

1. KDE(核密度估计)分布图

   当我们一想到要对比训练集和测试集的分布，便是画概率密度函数直方图，但直方图看分布有两点缺陷:  受bin宽度影响大和不平滑，因此多数人会偏向于使用核密度估计图(Kernel Density Estimation,  KDE)，KDE是非参数检验，用于估计分布未知的密度函数，相比于直方图，它受bin影响更小，绘图呈现更平滑，易于对比数据分布。

   ```python
   import numpy as np
   import seaborn as sns
   import matplotlib.pyplot as plt
   
   # 创建样例特征
   train_mean, train_cov = [0, 2], [(1, .5), (.5, 1)]
   test_mean, test_cov = [0, .5], [(1, 1), (.6, 1)]
   train_feat, _ = np.random.multivariate_normal(train_mean, train_cov, size=50).T
   test_feat, _ = np.random.multivariate_normal(test_mean, test_cov, size=50).T
   
   # 绘KDE对比分布
   sns.kdeplot(train_feat, shade = True, color='r', label = 'train')
   sns.kdeplot(test_feat, shade = True, color='b', label = 'test')
   plt.xlabel('Feature')
   plt.legend()
   plt.show()
   ```

2. KS检验

   KDE是PDF来对比，而KS检验是基于CDF(累计分布函数Cumulative Distribution  Function)来检验两个数据分布是否一致，它也是非参数检验方法(即不知道数据分布情况)。两条不同数据集下的CDF曲线，它们最大垂直差值可用作描述分布差异。

   调用scipy.stats.ks_2samp()[6]可轻松得到KS的统计值(最大垂直差)和假设检验下的p值：

   ```python
   from scipy import stats
   stats.ks_2samp(train_feat, test_feat)
   输出：KstestResult(statistic=0.2, pvalue=0.2719135601522248)
   ```

   若KS统计值小且p值大，则我们可以接受KS检验的原假设H0，即两个数据分布一致。上面样例数据的统计值较低，p值大于10%但不是很高，因此反映分布略微不一致。注意: p值<0.01，强烈建议拒绝原假设H0，p值越大，越倾向于原假设H0成立。

3. 对抗检验

   对抗验证是个很有趣的方法，它的思路是：我们构建一个分类器去分类训练集和测试集，如果模型能清楚分类，说明训练集和测试集存在明显区别(即分布不一致)，否则反之。具体步骤如下:

   - 训练集和测试集合并，同时新增标签‘Is_Test’去标记训练集样本为0，测试集样本为1。
   - 构建分类器(例如LGB, XGB等)去训练混合后的数据集(可采用交叉验证的方式)，拟合目标标签‘Is_Test’。
   - 输出交叉验证中最优的AUC分数。AUC越大(越接近1)，越说明训练集和测试集分布不一致。

4. overlap rate

   对于类别型变量检验同分布，我们可以对其进行编码然后KS检测，或者选择通过特征重合率来进行检测，通过特征重合率检测的思想是检测训练集特征在测试集中出现的比率，举个例子：



#### 解决方法

1. 构造合适的验证集

   当出现训练集和测试集分布不一致的，我们可以试图去构建跟测试集分布近似相同的验证集，保证线下验证跟线上测试分数不会抖动，这样我们就能得到稳定的benchmark。Qiuyan918在基于对抗验证的基础上，提出了三种构造合适的验证集的办法：

   - 人工划分验证集
   
   - 选择和测试集最相似的样本作为验证集

     前面在讲对抗验证时，我们有训练出一个分类器去分类训练集和测试集，那么自然我们也能预测出训练集属于测试集的概率(即训练集在‘Is_Test’标签下预测概率)，我们对训练集的预测概率进行降序排列，选择概率最大的前20%样本划分作为验证集，这样我们就能从原始数据集中，得到分布跟测试集接近的一个验证集了。之后，我们还可以评估划分好的验证集跟测试集的分布状况，评估方法：将验证集和测试集做对抗验证，若AUC越小，说明划分出的验证集和测试集分布越接近(即分类器越分不清验证集和测试集)。

   - 有权重的交叉验证

     如果我们对训练集里分布更偏向于测试集分布的样本更大的样本权重，给与测试集分布不太一致的训练集样本更小权重，也能一定程度上，帮助我们线下得到不易抖动的评估分数。在lightgbm库的Dataset初始化参数中，便提供了样本加权的参数weight。对抗验证的分类器预测训练集的Is_Test概率作为权重即可。
   
2. 删除分布不一致特征

3. 修改分布不一致的特征输入

   当我们对比观察训练集和测试集的KDE时，若发现对数据做数学运算(例如加减乘除)或对增删样本就能修正分布，使得分布接近一致，那么我们可以试试。比如，蚂蚁金服比赛里，亚军团队发现"用户交易请求"特征在训练集中包含0、1和-1，而测试集只有1和0样本，因此他们对训练集删去了特征值为-1的样本，减少该特征在训练集和测试集的差异[9]。

4. 修正分布不一致的预测输出

   除了对输入特征进行分布检查，我们也可以检查目标特征的分布，看是否存在可修正的空间。这种案例很少见，因为你看不到测试集的目标特征值。在“AI  Earth”人工智能创新挑战赛里，我们有提到官方提供两类数据集作为训练集，分别是CMIP模拟数据和SODA真实数据，然后测试集又是SODA真实数据，其中前排参赛者YueTan就将CMIP和SODA的目标特征分布画在一起，然后发现SODA的值更集中，且整体分布偏右一些，所以对用CMIP训练得到的预测值加了一个小的常数，修正CMIP下模型的预测输出，使得它分布更偏向于SODA分布。

4. 伪标签

   伪标签是半监督方法，利用未标注数据加入训练，我们先看看伪标签的思路，再讨论为什么它可能在一定程度上对分布不一致的数据集有帮助。伪标签最常见的方法是：
   
   - 使用有标注的训练集训练模型M;
   - 然后用模型M预测未标注的测试集;
   - 选取测试集中预测置信度高的样本加入训练集中;
   - 使用标注样本和高置信度的预测样本训练模型M';
   - 预测测试集，输出预测结果。
   
   由上图我们可以看到，模型的训练引入了部分测试集的样本，这样相当于引入了部分测试集的分布。但需要注意：
   
   1. 相比于前面的方法，伪标签通常没有表现的很好，因为它引入的是置信度高的测试集样本，这些样本很可能跟训练集分布接近一致，所以才会预测概率高。因此引入的测试集分布也没有很不同，所以使用时常发生过拟合的情况。
   
   2. 注意引入的是高置信度样本，如果引入低置信度样本，会带来很大的噪声。另外，高置信度样本也不建议选取过多加入训练集，这也是为了避免模型过拟合。
   
   3. 伪标签适用于图像领域更多些，表格型比赛建议最后没办法再考虑该方法，因为本人使用过该方法，涨分的可能性都不是很高(也可能是我没用好)。